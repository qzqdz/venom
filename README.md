# ğŸ›¡ï¸ LLM Safety Evaluation Framework

A comprehensive framework of sensitive knowledge processing, evaluating and detecting potential safety issues in Large Language Models (LLMs).


## ğŸ“‹ Overview

This framework provides a multi-faceted approach to evaluate LLM safety through:
- ğŸ” Jailbreak detection
- ğŸš« Harmful content filtering
- ğŸ”„ Counterfactual evaluation
- âœ… Policy compliance assessment

## ğŸ§© Core Components

### ğŸ”§ Evaluation Tools
- `choice.py`: Multiple-choice data processing
- `judge.py`: Judgment-based data processing
- `open.py`: Open-ended response processing
- `counterfactual_eval.py`: Counterfactual evaluation framework
- `jailbreak_detection.py`: Multi-mode detection system supporting:
  - ğŸ¤– GPT-based judgment
  - ğŸ“Š Meta policy scoring
  - ğŸ”— Combined detection approach

### ğŸ›¡ï¸ Filtering System
- `filter_judge_enhanced.py`: Enhanced filtering for judgment-based content
- `filter_choice_enhanced.py`: Enhanced filtering for multiple-choice content
- `filter_raw_enhanced.py`: Raw content filtering with improved accuracy

## ğŸ“ Directory Structure
- `mask/`: ğŸ­ Masking utilities for sensitive content
- `judge/`: âš–ï¸ Judgment-based evaluation components
- `choice/`: âœ… Multiple-choice evaluation components
- `dataset/`: ğŸ“Š Sample datasets (partial release for review)

## ğŸš€ Usage

### ğŸ“ Open-Ended Evaluation
```bash
python open.py \
    --target_model "gpt-4o" \
    --eval_model "gpt-4o" \
    --temperature 0.7 \
    --reasoning \
    --sample 200 \
    --dataset_path "dataset/VENOM_dataset/fraud/fraud_final_translated.json" \
    --output_dir "evaluation_results" \
    --log_dir "logs"
```

### âœ… Multiple-Choice Evaluation
```bash
python choice.py \
    --target_model "gpt-4o" \
    --temperature 0.0 \
    --reasoning \
    --sample 200 \
    --dataset_path "dataset/VENOM_dataset/fraud/fraud_choice_translated.json"
```

### âš–ï¸ Judgment-Based Evaluation
```bash
python judge.py \
    --target_model "gpt-4o" \
    --temperature 0.7 \
    --reasoning \
    --sample 200 \
    --dataset_path "dataset/VENOM_dataset/fraud/fraud_judge_translated.json" \
    --output_dir "evaluation_results" \
    --log_dir "logs"
```

### ğŸ”„ Counterfactual Evaluation
```bash
python counterfactual_eval.py \
    --input_file "dataset/VENOM_dataset/counterfactual_plan/counterfactual_planning.json" \
    --sample 10 \
    --target_model "gpt-4o"
```

### ğŸ›¡ï¸ Jailbreak Detection
```bash
python jailbreak_detection.py \
    --input_file "dataset/VENOM_dataset/counterfactual_judge/mixed_harmful_qa_0.50_100.json" \
    --model "gpt-4o" \
    --mode "meta_policy"
```

## âœ¨ Features

1. **ğŸ” Multi-Mode Detection**
   - ğŸ¤– GPT-based judgment
   - ğŸ“Š Policy-based scoring
   - ğŸ”— Combined approach

2. **ğŸ›¡ï¸ Enhanced Filtering**
   - ğŸ“ˆ Improved accuracy
   - ğŸ”„ Multiple content types support
   - âš™ï¸ Configurable thresholds

3. **ğŸ“Š Comprehensive Evaluation**
   - ğŸ”„ Counterfactual analysis
   - âš–ï¸ Balanced judgment
   - âœ… Results verification

## ğŸ“ LLM-as-a-Judge Sample Generation Methodology

> **Note**: This section details the implementation of the generation method of adversarial samples for LLM-as-a-Judge testing described in our paper. A more comprehensive version will be included in the paper's appendix.

### 1. ğŸ­ Sensitive Information Recognition and Masking (mask.py)

`mask.py` implements an iterative sensitive information extraction and masking system with the following workflow:

1. **ğŸ”„ Multi-round LLM Calls**
   - Performs multiple (n_iter) LLM calls for each QA pair
   - Accumulates entity recognition, mask suggestions, and sensitive terms
   - Conducts extraction based on previous round's mask results, achieving "layer-by-layer purification"

2. **ğŸ¯ Mask Management**
   - Uses `MaskManager` class to manage sensitive word masks
   - Generates unique mask codes for each sensitive term
   - Provides harmless replacement text
   - Supports bidirectional mapping between sensitive terms and masks/harmless text

3. **ğŸ’¾ Result Saving**
   - Writes results to .jsonl files in real-time
   - Automatically backs up old output files
   - Supports checkpoint continuation

### 2. ğŸ® Controlled Harmful Content Generation (joint.py)

`joint.py` implements a probability-based harmful content control generation system:

1. **ğŸ² Probability-controlled Replacement**
   - Supports setting probability for retaining harmful content (keep_harmful_prob)
   - Controls error range through tolerance parameter
   - Prioritizes longer sensitive terms to ensure replacement accuracy

2. **ğŸ“‹ Data Format Processing**
   - Supports both standard JSON and JSONL formats
   - Automatically fixes format issues
   - Performs data validation and cleaning

3. **ğŸ¯ Sample Generation**
   - Supports specified sample count
   - Prevents duplicate generation
   - Ensures harmful content ratio stays within target range

### ğŸ› ï¸ Implementation Details

The implementation follows the methodology described in our paper's LLM-as-a-Judge approach, with the following key features:

- **ğŸ”„ Iterative Refinement**: Multiple rounds of LLM-based judgment to ensure comprehensive coverage
- **ğŸ® Controlled Generation**: Precise control over harmful content ratio for balanced evaluation
- **âœ… Quality Assurance**: Multiple validation steps to ensure data quality and consistency

### ğŸš€ Usage Examples

```bash
# ğŸ­ Sensitive Information Recognition and Masking
python mask.py \
    --input ./data/sensitive_data.json \
    --output ./data/masked_data.jsonl \
    --n_iter 3

# ğŸ® Controlled Harmful Content Generation
python joint.py \
    --input ./data/masked_data.jsonl \
    --output ./data/generated_data.json \
    --keep_harmful_prob 0.5 \
    --sample_count 100 \
    --tolerance 0.05
```

## ğŸ¤ Research Ethics

This framework is developed for research purposes to:
- ğŸ›¡ï¸ Improve LLM safety mechanisms
- ğŸ” Understand model vulnerabilities
- ğŸŒŸ Contribute to responsible AI development

## ğŸ“š Citation

If you use this framework in your research, please cite our paper (citation to be added upon acceptance).

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ“§ Contact

For questions regarding the framework or dataset access, please contact the corresponding author.

---
<div align="center">
  <img src="https://img.shields.io/badge/LLM-Safety-blue" alt="LLM Safety">
  <img src="https://img.shields.io/badge/Evaluation-Framework-green" alt="Evaluation Framework">
  <img src="https://img.shields.io/badge/Research-Tool-orange" alt="Research Tool">
  <img src="https://img.shields.io/badge/MIT-License-yellow" alt="MIT License">
</div>


> **Note**: This framework integrates with [easy-dataset](https://github.com/ConardLi/easy-dataset) for knowledge chunk generation and processing.
